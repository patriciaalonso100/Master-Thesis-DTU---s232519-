{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4141a4c6-0b84-4352-afbe-555157f2a358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json, hashlib, random, time, sys, platform\n",
    "from dataclasses import dataclass, asdict, replace\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import heapq\n",
    "import hashlib\n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "from dataclasses import replace\n",
    "\n",
    "from dataclasses import replace\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe0584-53b5-46da-8a87-bf069178d6c8",
   "metadata": {},
   "source": [
    "# BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1630b6fa-b231-435e-81b5-96b603eb741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BaselineConfig:\n",
    "    # Data / split\n",
    "    golden_id: str = \"episode_016\"\n",
    "    dataset_path: str = \"episodes_all_baseline.parquet\"   \n",
    "    alert_csv_path: str = \"Golden/Alert_1.csv\"         \n",
    "\n",
    "    # Feature extraction\n",
    "    max_text_features: int = 5000\n",
    "    min_df: int = 5\n",
    "    ngram_range: tuple = (1, 2)\n",
    "    text_col: str = \"masked_message_cl\"\n",
    "    cat_cols: tuple = ()\n",
    "\n",
    "    # Isolation Forest\n",
    "    if_n_estimators: int = 100\n",
    "    if_contamination: str = \"auto\"\n",
    "    if_random_state: int = 42\n",
    "\n",
    "    # Graph\n",
    "    graph_max_gap_host: int = 60\n",
    "    graph_max_gap_actor: int = 120              \n",
    "    use_host_edges: bool = True          \n",
    "    use_actor_edges: bool = True\n",
    "\n",
    "    # Alert attach\n",
    "    attach_strategy: str = \"prefer_sshd_success_then_closest\"\n",
    "\n",
    "    # RCA walk\n",
    "    rca_max_nodes: int = 200\n",
    "    rca_max_hops: int | None = None\n",
    "    rca_max_back_seconds: int = 30 * 60\n",
    "    rca_forward_slack_seconds: int = 60\n",
    "    rca_priority_mode: str = \"baseline\"\n",
    "\n",
    "    # Evaluation @k\n",
    "    ks: tuple = (5, 10, 20, 50)\n",
    "\n",
    "cfg_baseline = BaselineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b8639f-01f5-4121-9bac-28ec776525c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = PROJECT_DIR / \"data\" / cfg_baseline.dataset_path\n",
    "assert dataset_path.exists(), f\"Missing dataset parquet: {dataset_path}\"\n",
    "\n",
    "episodes_df = pd.read_parquet(dataset_path)\n",
    "episodes_df = episodes_df.reset_index(drop=True)\n",
    "episodes_df[\"node_id\"] = episodes_df.index.astype(int)\n",
    "\n",
    "dataset_fingerprint = file_sha256_12(dataset_path)\n",
    "\n",
    "cfg_fingerprint = hashlib.sha256(json.dumps(asdict(cfg_baseline), sort_keys=True).encode(\"utf-8\")).hexdigest()[:12]\n",
    "RUN_DIR = RUNS_DIR / f\"run_{cfg_fingerprint}_{dataset_fingerprint}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_json(asdict(cfg_baseline), RUN_DIR / \"baseline_config.json\")\n",
    "save_json(get_env_info(), RUN_DIR / \"environment.json\")\n",
    "save_json({\"dataset_path\": str(dataset_path), \"dataset_sha256_12\": dataset_fingerprint}, RUN_DIR / \"dataset_fingerprint.json\")\n",
    "\n",
    "print(\"Loaded:\", dataset_path)\n",
    "print(\"Rows:\", len(episodes_df))\n",
    "print(\"Run dir:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d20d21-ec6a-43e0-8351-64bd386aa09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_DIR = NOTEBOOK_DIR.parent\n",
    "dataset_path = PROJECT_DIR / \"data\" / cfg_baseline.dataset_path\n",
    "assert dataset_path.exists(), f\"Missing dataset parquet: {dataset_path}\"\n",
    "\n",
    "episodes_df = pd.read_parquet(dataset_path)\n",
    "episodes_df = episodes_df.reset_index(drop=True)\n",
    "episodes_df[\"node_id\"] = episodes_df.index.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f93531a9-7eb1-404d-9ebd-865341901f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    29998.000000\n",
      "mean        -0.117049\n",
      "std          0.034239\n",
      "min         -0.154611\n",
      "25%         -0.139395\n",
      "50%         -0.128928\n",
      "75%         -0.101724\n",
      "max          0.109014\n",
      "Name: baseline_score_iso, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class BaselineFeatureBuilder:\n",
    "    cfg_baseline: BaselineConfig\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.text_vectorizer = TfidfVectorizer(\n",
    "            max_features=self.cfg_baseline.max_text_features,\n",
    "            min_df=self.cfg_baseline.min_df,\n",
    "            ngram_range=self.cfg_baseline.ngram_range,\n",
    "        )\n",
    "\n",
    "        transformers = [\n",
    "            (\"text\", self.text_vectorizer, self.cfg_baseline.text_col),\n",
    "        ]\n",
    "\n",
    "        cat_cols = list(self.cfg_baseline.cat_cols) if getattr(self.cfg_baseline, \"cat_cols\", None) else []\n",
    "        if len(cat_cols) > 0:\n",
    "            try:\n",
    "                self.cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "            except TypeError:\n",
    "                self.cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "            transformers.append((\"cat\", self.cat_encoder, cat_cols))\n",
    "        \n",
    "        self.preprocessor = ColumnTransformer(transformers=transformers)\n",
    "        self.pipeline = Pipeline(steps=[(\"preprocess\", self.preprocessor)])\n",
    "\n",
    "    def fit_transform(self, df: pd.DataFrame) -> sparse.csr_matrix:\n",
    "        return self.pipeline.fit_transform(df)\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> sparse.csr_matrix:\n",
    "        return self.pipeline.transform(df)\n",
    "\n",
    "train_df = episodes_df[episodes_df[\"episode_id\"] != cfg_baseline.golden_id].copy()\n",
    "golden_df = episodes_df[episodes_df[\"episode_id\"] == cfg_baseline.golden_id].copy()\n",
    "\n",
    "train_df[cfg_baseline.text_col] = train_df[cfg_baseline.text_col].fillna(\"\").astype(str)\n",
    "golden_df[cfg_baseline.text_col] = golden_df[cfg_baseline.text_col].fillna(\"\").astype(str)\n",
    "\n",
    "assert len(golden_df) > 0, f\"No rows for golden_id={cfg_baseline.golden_id}\"\n",
    "\n",
    "feat_builder = BaselineFeatureBuilder(cfg_baseline=cfg_baseline)\n",
    "X_train = feat_builder.fit_transform(train_df)\n",
    "X_gold  = feat_builder.transform(golden_df)\n",
    "\n",
    "iso = IsolationForest(\n",
    "    n_estimators=cfg_baseline.if_n_estimators,\n",
    "    contamination=cfg_baseline.if_contamination,\n",
    "    random_state=cfg_baseline.if_random_state,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "iso.fit(X_train)\n",
    "\n",
    "raw_scores = iso.decision_function(X_gold)   # higher = more normal\n",
    "golden_df[\"baseline_score_iso\"] = -raw_scores  # higher = more suspicious\n",
    "\n",
    "episodes_df.loc[golden_df.index, \"baseline_score_iso\"] = golden_df[\"baseline_score_iso\"].values\n",
    "print(golden_df[\"baseline_score_iso\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625f8e9d-f6fb-4ca6-bb5b-31fbfc16ac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_score(df: pd.DataFrame, col: str = \"baseline_score_iso\", out_col: str = \"score_norm\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    s = df[col]\n",
    "    mask = s.notna()\n",
    "    if mask.sum() == 0:\n",
    "        df[out_col] = 0.0\n",
    "        return df\n",
    "    s_valid = s[mask]\n",
    "    s_min, s_max = float(s_valid.min()), float(s_valid.max())\n",
    "    if s_max == s_min:\n",
    "        df[out_col] = 0.5\n",
    "        return df\n",
    "    score = (s - s_min) / (s_max - s_min)\n",
    "    score[~mask] = 0.0\n",
    "    df[out_col] = score\n",
    "    return df\n",
    "\n",
    "golden_df = add_normalized_score(golden_df, col=\"baseline_score_iso\", out_col=\"score_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae9eb297-0603-45fd-b62e-4fe215402a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_df.loc[golden_df.index, \"score_norm\"] = golden_df[\"score_norm\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1676ac2-d4d1-4321-ba6e-358b77914c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden graph: 29998 nodes, 33485 edges\n"
     ]
    }
   ],
   "source": [
    "def build_episode_graph(df: pd.DataFrame, cfg_baseline: BaselineConfig) -> nx.DiGraph:\n",
    "    df = df.sort_values(\"timestamp\").copy()\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        nid = int(row[\"node_id\"])\n",
    "        G.add_node(\n",
    "            nid,\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            stream=row[\"stream\"],\n",
    "            masked_message_cl=row[\"masked_message_cl\"],\n",
    "            actor_ip_anon=row.get(\"actor_ip_anon\", \"none\"),\n",
    "            host_anon=row.get(\"host_anon\", \"none\"),\n",
    "            baseline_score_iso=float(row.get(\"baseline_score_iso\", np.nan)),\n",
    "            score_norm=float(row.get(\"score_norm\", 0.0)),\n",
    "        )\n",
    "\n",
    "    def add_temporal_edges_for_key(key_col: str, max_gap: int, edge_type: str):\n",
    "        for key, group in df.groupby(key_col):\n",
    "            if str(key) in (\"none\", \"\", \"nan\", \"None\"):\n",
    "                continue\n",
    "            group = group.sort_values(\"timestamp\")\n",
    "            prev_nid, prev_ts = None, None\n",
    "            for _, r in group.iterrows():\n",
    "                nid = int(r[\"node_id\"])\n",
    "                ts = r[\"timestamp\"]\n",
    "                if prev_nid is not None:\n",
    "                    dt = (ts - prev_ts).total_seconds()\n",
    "                    if 0 <= dt <= max_gap:\n",
    "                        G.add_edge(prev_nid, nid, kind=edge_type, dt=float(dt))\n",
    "                prev_nid, prev_ts = nid, ts\n",
    "\n",
    "    if cfg_baseline.use_actor_edges:\n",
    "        add_temporal_edges_for_key(\"actor_ip_anon\", cfg_baseline.graph_max_gap_actor, \"actor_ip_temporal\")\n",
    "    if cfg_baseline.use_host_edges:\n",
    "        add_temporal_edges_for_key(\"host_anon\", cfg_baseline.graph_max_gap_host, \"host_temporal\")\n",
    "\n",
    "    return G\n",
    "\n",
    "G_golden = build_episode_graph(golden_df, cfg_baseline)\n",
    "print(\"Golden graph:\", G_golden.number_of_nodes(), \"nodes,\", G_golden.number_of_edges(), \"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c3c583c-6d41-4e8b-8c08-63b3e654a091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(obj: Any, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2, ensure_ascii=False, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0574d827-02a2-4f43-924f-01fb78769f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attached node_ids: [560913]\n"
     ]
    }
   ],
   "source": [
    "ALERT_NODE_ID = \"ALERT_NODE_ID\"\n",
    "\n",
    "def short_hash(x):\n",
    "    return \"none\" if pd.isna(x) or not str(x).strip() else hashlib.sha256(str(x).encode()).hexdigest()[:12]\n",
    "\n",
    "alert_path = PROJECT_DIR / cfg_baseline.alert_csv_path\n",
    "assert alert_path.exists(), f\"Missing alert csv: {alert_path}\"\n",
    "alert_df = pd.read_csv(alert_path)\n",
    "alert_row = alert_df.iloc[0]\n",
    "\n",
    "alert_ts = pd.to_datetime(alert_row[\"@timestamp\"], utc=True)\n",
    "alert_src_ip = alert_row.get(\"source.ip\", None)\n",
    "alert_name   = alert_row.get(\"kibana.alert.rule.name\", \"\")\n",
    "alert_reason = alert_row.get(\"kibana.alert.reason\", \"\")\n",
    "alert_desc   = alert_row.get(\"description\", \"\")\n",
    "\n",
    "def attach_alert_node(G: nx.DiGraph, df: pd.DataFrame, alert_ts: pd.Timestamp, alert_src_ip=None,\n",
    "                      alert_name: str = \"\", alert_reason: str = \"\", alert_desc: str = \"\") -> list[int]:\n",
    "    G.add_node(\n",
    "        ALERT_NODE_ID,\n",
    "        timestamp=alert_ts,\n",
    "        stream=\"alert\",\n",
    "        masked_message_cl=f\"[alert] name={alert_name} reason={alert_reason} desc={alert_desc}\",\n",
    "        baseline_score_iso=0.0,\n",
    "        score_norm=1.0,\n",
    "    )\n",
    "\n",
    "    attacker_ip_str = None\n",
    "    if alert_src_ip is not None:\n",
    "        attacker_ip_str = str(alert_src_ip).strip()\n",
    "        if not attacker_ip_str or attacker_ip_str.lower() in (\"nan\", \"none\"):\n",
    "            attacker_ip_str = None\n",
    "\n",
    "    df_sorted = df.sort_values(\"timestamp\").copy()\n",
    "    df_sorted[\"dt_abs\"] = (df_sorted[\"timestamp\"] - alert_ts).abs().dt.total_seconds()\n",
    "\n",
    "    best = pd.DataFrame()\n",
    "\n",
    "    # 1) prefer sshd success from attacker ip\n",
    "    if attacker_ip_str is not None:\n",
    "        attacker_hash = short_hash(attacker_ip_str)\n",
    "        cand1 = df_sorted[\n",
    "            (df_sorted[\"stream\"] == \"system.auth\") &\n",
    "            (df_sorted[\"masked_message_cl\"].astype(str).str.contains(\"out=success\", na=False)) &\n",
    "            (df_sorted[\"masked_message_cl\"].astype(str).str.contains(\"proc=sshd\", na=False)) &\n",
    "            (df_sorted.get(\"actor_ip_anon\", \"\") == attacker_hash)\n",
    "        ]\n",
    "        if not cand1.empty:\n",
    "            best = cand1.nsmallest(1, \"dt_abs\")\n",
    "\n",
    "    # 2) else any sshd success\n",
    "    if best.empty:\n",
    "        cand2 = df_sorted[\n",
    "            (df_sorted[\"stream\"] == \"system.auth\") &\n",
    "            (df_sorted[\"masked_message_cl\"].astype(str).str.contains(\"out=success\", na=False)) &\n",
    "            (df_sorted[\"masked_message_cl\"].astype(str).str.contains(\"proc=sshd\", na=False))\n",
    "        ]\n",
    "        if not cand2.empty:\n",
    "            best = cand2.nsmallest(1, \"dt_abs\")\n",
    "\n",
    "    # 3) else closest event\n",
    "    if best.empty:\n",
    "        best = df_sorted.nsmallest(1, \"dt_abs\")\n",
    "\n",
    "    attached = [int(best.iloc[0][\"node_id\"])]\n",
    "    for nid in attached:\n",
    "        dt = (G.nodes[nid][\"timestamp\"] - alert_ts).total_seconds()\n",
    "        G.add_edge(ALERT_NODE_ID, nid, kind=\"alert_to_log\", dt=float(dt))\n",
    "\n",
    "    return attached\n",
    "\n",
    "attached_log_ids = attach_alert_node(G_golden, golden_df, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "print(\"Attached node_ids:\", attached_log_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d78a3a-03b4-4bfc-b1d7-b5987aa3804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RCA subgraph:\n",
      "  Nodes: 200\n",
      "  Edges: 227\n"
     ]
    }
   ],
   "source": [
    "def rca_walk(G: nx.DiGraph, cfg_baseline: BaselineConfig, alert_node_id: str = ALERT_NODE_ID) -> tuple[nx.DiGraph, set, list]:\n",
    "    alert_ts = G.nodes[alert_node_id][\"timestamp\"]\n",
    "\n",
    "    def get_score_norm(node_id):\n",
    "        if node_id == alert_node_id:\n",
    "            return 1.0\n",
    "        try:\n",
    "            return float(G.nodes[node_id].get(\"score_norm\", 0.0))\n",
    "        except Exception:\n",
    "            return 0.0\n",
    "\n",
    "    selected = set([alert_node_id])\n",
    "    visited  = set([alert_node_id])\n",
    "    ranked   = [alert_node_id]\n",
    "\n",
    "    heap = []\n",
    "    for succ in G.successors(alert_node_id):\n",
    "        ts = G.nodes[succ][\"timestamp\"]\n",
    "        dt = (alert_ts - ts).total_seconds()\n",
    "        if dt > cfg_baseline.rca_max_back_seconds:\n",
    "            continue\n",
    "        if dt < -cfg_baseline.rca_forward_slack_seconds:\n",
    "            continue\n",
    "        score = get_score_norm(succ)\n",
    "        heapq.heappush(heap, ((-score, abs(dt)), 1, succ))\n",
    "\n",
    "    while heap and len(selected) < cfg_baseline.rca_max_nodes:\n",
    "        (_, hops, node_id) = heapq.heappop(heap)\n",
    "        if node_id in visited:\n",
    "            continue\n",
    "        if cfg_baseline.rca_max_hops is not None and hops > cfg_baseline.rca_max_hops:\n",
    "            continue\n",
    "\n",
    "        visited.add(node_id)\n",
    "        selected.add(node_id)\n",
    "        ranked.append(node_id)\n",
    "\n",
    "        for pred in G.predecessors(node_id):\n",
    "            if pred in visited or pred == alert_node_id:\n",
    "                continue\n",
    "            ts = G.nodes[pred][\"timestamp\"]\n",
    "            dt = (alert_ts - ts).total_seconds()\n",
    "            if dt > cfg_baseline.rca_max_back_seconds:\n",
    "                continue\n",
    "            if dt < -cfg_baseline.rca_forward_slack_seconds:\n",
    "                continue\n",
    "            score = get_score_norm(pred)\n",
    "            heapq.heappush(heap, ((-score, abs(dt)), hops + 1, pred))\n",
    "\n",
    "    subG = G.subgraph(selected).copy()\n",
    "    return subG, selected, ranked\n",
    "\n",
    "subG_baseline, selected_nodes_baseline, rca_ranked_baseline = rca_walk(G_golden, cfg_baseline, alert_node_id=ALERT_NODE_ID)\n",
    "print(\"RCA subgraph:\")\n",
    "print(\"  Nodes:\", subG_baseline.number_of_nodes())\n",
    "print(\"  Edges:\", subG_baseline.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519c3be6-285b-4d7c-a556-6c22466e0e31",
   "metadata": {},
   "source": [
    "# CONTRASTIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28cb40b6-acce-4f85-9345-d96aed47cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ContrastiveRunConfig:\n",
    "    # Data\n",
    "    golden_id: str = \"episode_016\"\n",
    "    dataset_path: str = \"episodes_all_baseline.parquet\"   \n",
    "    text_col: str = \"masked_message_cl\"\n",
    "    alert_csv_path: str = \"Golden/Alert_1.csv\"\n",
    "\n",
    "    # Pair mining\n",
    "    pair_window_sec: int = 90\n",
    "    max_pos_per_anchor: int = 2\n",
    "    max_anchors_per_episode: int = 10000 \n",
    "    max_neg_per_anchor: int = 2\n",
    "\n",
    "    # Feature base\n",
    "    tfidf_max_features: int = 5000\n",
    "    tfidf_min_df: int = 5\n",
    "    tfidf_ngram_range: tuple = (1, 2)\n",
    "    svd_dim = 256\n",
    "    svd_dim_list = [64, 128, 256, 512]\n",
    "    normalize_svd = True         \n",
    "\n",
    "    # Graph\n",
    "    graph_max_gap_actor: int = 2 * 60   \n",
    "    graph_max_gap_host: int  = 60            \n",
    "    use_host_edges: bool = True          \n",
    "    use_actor_edges: bool = True\n",
    "\n",
    "    # RCA walk\n",
    "    rca_max_nodes: int = 200\n",
    "    rca_max_hops: int | None = None\n",
    "    rca_max_back_seconds: int = 30 * 60\n",
    "    rca_forward_slack_seconds: int = 60\n",
    "    rca_priority_mode: str = \"baseline\"\n",
    "\n",
    "    # Contrastive training\n",
    "    proj_dim: int = 128\n",
    "    batch_size: int = 512\n",
    "    epochs: int = 5  \n",
    "    lr: float = 1e-4  \n",
    "    temperature: float = 0.07\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    normalize_proj = True          \n",
    "\n",
    "    # Scoring\n",
    "    score_merge_w: float = 1.0\n",
    "\n",
    "    # Evaluation @k\n",
    "    ks: tuple = (5, 10, 20, 50)\n",
    "\n",
    "cfg = ContrastiveRunConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "378d3cc9-1196-479e-bdba-7f0911cacbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 569803\n",
      "Train rows: 539805 Golden rows: 29998\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_DIR = NOTEBOOK_DIR.parent\n",
    "DATASET_DIR = PROJECT_DIR / \"data\"\n",
    "RUNS_DIR = PROJECT_DIR / \"runs\" / \"contrastive\"\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(cfg.seed)\n",
    "\n",
    "DATA_PATH = DATASET_DIR / cfg.dataset_path\n",
    "assert DATA_PATH.exists(), f\"Missing: {DATA_PATH}\"\n",
    "\n",
    "df = pd.read_parquet(DATA_PATH).reset_index(drop=True)\n",
    "df[\"node_id\"] = df.index.astype(int)\n",
    "\n",
    "train_df = df[df[\"episode_id\"] != cfg.golden_id].copy()\n",
    "gold_df  = df[df[\"episode_id\"] == cfg.golden_id].copy()\n",
    "\n",
    "df[cfg.text_col] = df[cfg.text_col].fillna(\"\").astype(str)\n",
    "train_df[cfg.text_col] = train_df[cfg.text_col].fillna(\"\").astype(str)\n",
    "gold_df[cfg.text_col] = gold_df[cfg.text_col].fillna(\"\").astype(str)\n",
    "\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(\"Train rows:\", len(train_df), \"Golden rows:\", len(gold_df))\n",
    "print(\"Device:\", cfg.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a346b2dd-7018-44ca-91f1-ef2cadfb09fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base embeddings: (539805, 256) (29998, 256) (169.02s)\n"
     ]
    }
   ],
   "source": [
    "# Base embeddings with TF-IDF\n",
    "t0 = time.time()\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=cfg.tfidf_max_features,\n",
    "    min_df=cfg.tfidf_min_df,\n",
    "    ngram_range=cfg.tfidf_ngram_range,\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(train_df[cfg.text_col])\n",
    "X_gold_tfidf  = tfidf.transform(gold_df[cfg.text_col])\n",
    "\n",
    "# SVD\n",
    "svd = TruncatedSVD(n_components=cfg.svd_dim, random_state=cfg.seed)\n",
    "Z_train = svd.fit_transform(X_train_tfidf)\n",
    "Z_gold  = svd.transform(X_gold_tfidf)\n",
    "\n",
    "#L2 normalization\n",
    "if cfg.normalize_svd:\n",
    "    norm = Normalizer(copy=False)\n",
    "    Z_train = norm.fit_transform(Z_train)\n",
    "    Z_gold  = norm.transform(Z_gold)\n",
    "\n",
    "print(\"Base embeddings:\", Z_train.shape, Z_gold.shape, f\"({time.time()-t0:.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494a48a-597f-4d62-976e-273580ffe4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_positive_pairs(ep: pd.DataFrame, window_sec: int, max_pos_per_anchor: int, max_anchors: int):\n",
    "    ep = ep.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "    n = len(ep)\n",
    "    if n == 0:\n",
    "        return []\n",
    "    anchor_idxs = np.random.RandomState(cfg.seed).choice(n, size=min(n, max_anchors), replace=False)\n",
    "    anchor_idxs = np.sort(anchor_idxs)          # events we will try to find a related partner for\n",
    "    ts = ep[\"timestamp\"].values.astype(\"datetime64[ns]\")\n",
    "    actor = ep[\"actor_ip_anon\"].astype(str).values\n",
    "    host = ep[\"host_anon\"].astype(str).values\n",
    "    stream = ep[\"stream\"].astype(str).values\n",
    "    pairs = []\n",
    "    for i in anchor_idxs:\n",
    "        t_i = ts[i]\n",
    "        lo = np.searchsorted(ts, t_i - np.timedelta64(window_sec, \"s\"), side=\"left\")\n",
    "        hi = np.searchsorted(ts, t_i + np.timedelta64(window_sec, \"s\"), side=\"right\")\n",
    "        if hi - lo <= 1:\n",
    "            continue\n",
    "        cand = np.arange(lo, hi)\n",
    "        cand = cand[cand != i]\n",
    "        if len(cand) == 0:\n",
    "            continue\n",
    "        ai = actor[i]\n",
    "        hi_i = host[i]\n",
    "        si = stream[i]\n",
    "        pos = []\n",
    "        # Prefer different stream, same actor\n",
    "        if ai not in (\"none\", \"nan\", \"\"):\n",
    "            same_actor = cand[actor[cand] == ai]\n",
    "            cross_stream = same_actor[stream[same_actor] != si]\n",
    "            pos.extend(list(cross_stream[:max_pos_per_anchor]))\n",
    "            # Fallback to same stream if needed\n",
    "            if len(pos) < max_pos_per_anchor:\n",
    "                same_stream = same_actor[stream[same_actor] == si]\n",
    "                pos.extend(list(same_stream[:max_pos_per_anchor - len(pos)]))\n",
    "        # Fallback: different stream, same host\n",
    "        if len(pos) < max_pos_per_anchor and hi_i not in (\"none\", \"nan\", \"\"):\n",
    "            same_host = cand[host[cand] == hi_i]\n",
    "            cross_stream = same_host[stream[same_host] != si]\n",
    "            for j in cross_stream:\n",
    "                if j not in pos:\n",
    "                    pos.append(int(j))\n",
    "                if len(pos) >= max_pos_per_anchor:\n",
    "                    break\n",
    "        for j in pos:\n",
    "            pairs.append((int(ep.iloc[i][\"node_id\"]), int(ep.iloc[j][\"node_id\"])))\n",
    "    return pairs\n",
    "\n",
    "t0 = time.time()\n",
    "all_pairs = []\n",
    "for eid, ep in train_df.groupby(\"episode_id\"): #mining happens within an episode\n",
    "    all_pairs.extend(\n",
    "        mine_positive_pairs(ep, cfg.pair_window_sec, cfg.max_pos_per_anchor, cfg.max_anchors_per_episode)\n",
    "    )\n",
    "all_pairs = list(dict.fromkeys(all_pairs)) \n",
    "print(\"Mined positive pairs:\", len(all_pairs), f\"({time.time()-t0:.2f}s)\")\n",
    "assert len(all_pairs) > 0, \"No positive pairs mined — increase window or check actor/host coverage.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98144d87-a9d6-4439-9429-a9b76e0817a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_node = train_df[[\"node_id\"]].reset_index(drop=True)\n",
    "node_to_idx = dict(zip(train_by_node[\"node_id\"].values, np.arange(len(train_by_node)))) \n",
    "\n",
    "pair_idx = []\n",
    "for a, p in all_pairs:     # a,p are node_id\n",
    "    ia = node_to_idx.get(a)\n",
    "    ip = node_to_idx.get(p)\n",
    "    if ia is not None and ip is not None:\n",
    "        pair_idx.append((ia, ip)) # This is a list of tuples with indices, no info here yet about events, just where to find them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e31f21-2ce7-4212-b012-49b5cf944313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairDataset(Dataset):\n",
    "    def __init__(self, Z: np.ndarray, pairs: list[tuple[int,int]]):\n",
    "        self.Z = torch.tensor(Z, dtype=torch.float32) \n",
    "        self.pairs = pairs\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        i, j = self.pairs[idx]\n",
    "        return self.Z[i], self.Z[j] # for the i-th pair, return the two float vectors that correspond to the anchor and its positive\n",
    "\n",
    "ds_pairs = PairDataset(Z_train, pair_idx)\n",
    "dl = DataLoader(ds_pairs, batch_size=cfg.batch_size, shuffle=True, drop_last=True) # Here we structure the pairs in mini-batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0269dc-bb42-473d-b241-09e96654e67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projector(nn.Module): # Here I map my base embedding space (SVD) into a space that is optimized for the contrastive objective\n",
    "    def __init__(self, in_dim: int, out_dim: int, normalize_out: bool = True):\n",
    "        super().__init__()\n",
    "        self.normalize_out = normalize_out\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, in_dim), # Creates a learned mixture of the original 256 numbers, so tries different weights for the other features\n",
    "                                       # and sees how the loss function says how the model behaves\n",
    "            nn.ReLU(), # turns to 0 negatives\n",
    "            nn.Linear(in_dim, out_dim), # This is the dimensionality reduction step\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        return F.normalize(z, dim=1) if self.normalize_out else z # We normalize for the similarity to be computed as cosine similarity\n",
    "\n",
    "\n",
    "def info_nce_loss(z1, z2, temperature: float):\n",
    "    B = z1.size(0)\n",
    "    labels = torch.arange(B, device=z1.device) # For a batch of size B, the index i is the positive match for index i\n",
    "    logits12 = (z1 @ z2.T) / temperature  # similarity computed with dot products, so for it to be cosine similarity we have normalized in Projector\n",
    "    logits21 = (z2 @ z1.T) / temperature\n",
    "    return 0.5 * (F.cross_entropy(logits12, labels) + F.cross_entropy(logits21, labels)) # For each i, make the biggest score in row i occur at column i (the true positive)\n",
    "\n",
    "proj = Projector(cfg.svd_dim, cfg.proj_dim, normalize_out=cfg.normalize_proj).to(cfg.device)\n",
    "\n",
    "opt = torch.optim.AdamW(proj.parameters(), lr=cfg.lr, weight_decay=1e-4) # Responsible for updating the projector’s learnable parameters (weights)\n",
    "                                                                         # torch.optim.AdamW(...) is AdamW optimization algorithm\n",
    "                                                                         # proj.parameters() is the weights of the linear layers\n",
    "                                                                         # lr is learning rate, a smaller one gives smaller, more cautious weight updates\n",
    "                                                                         # weight_decay=1e-4: \"prefer simpler parameter values unless there is strong evidence to make them large.”\n",
    "\n",
    "proj.train()\n",
    "for ep in range(cfg.epochs):\n",
    "    losses = []\n",
    "    for x1, x2 in dl:\n",
    "        x1, x2 = x1.to(cfg.device), x2.to(cfg.device)   # Here we move mini-batches to GPU, not CPU\n",
    "        z1, z2 = proj(x1), proj(x2)\n",
    "        loss = info_nce_loss(z1, z2, cfg.temperature)\n",
    "\n",
    "        opt.zero_grad()  # Reset gradients to 0 to not be summing gradients across multiple batches\n",
    "        loss.backward()  # Computes gradients of loss: “If I increase this weight slightly, does the loss go up or down, and by how much?”\n",
    "        torch.nn.utils.clip_grad_norm_(proj.parameters(), 1.0)  # Computes the overall norm (size) of all gradients in the projector, and if it is > 1.0, it scales all gradients down so the norm becomes 1.0\n",
    "        opt.step() # Optimization step, updates the weights\n",
    "\n",
    "        losses.append(loss.item())\n",
    "    print(f\"Epoch {ep+1}/{cfg.epochs} loss={np.mean(losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c612c0-af72-41d0-b357-028acdc6502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "RUN_DIR = RUNS_DIR / f\"run_seed{cfg.seed}_svd{cfg.svd_dim}_proj{cfg.proj_dim}_win{cfg.pair_window_sec}\"\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "torch.save(proj.state_dict(), RUN_DIR / \"projector.pt\")\n",
    "with (RUN_DIR / \"config.json\").open(\"w\") as f:\n",
    "    json.dump(asdict(cfg), f, indent=2)\n",
    "print(\"Saved:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580032b-f037-41ea-92f1-e8ec794a8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj.eval()\n",
    "with torch.no_grad():                    \n",
    "    Zt = torch.tensor(Z_train, dtype=torch.float32, device=cfg.device)\n",
    "    Zg = torch.tensor(Z_gold, dtype=torch.float32, device=cfg.device)\n",
    "    E_train = proj(Zt).cpu().numpy()\n",
    "    E_gold  = proj(Zg).cpu().numpy()\n",
    "\n",
    "print(\"Projected embeddings:\", E_train.shape, E_gold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf663076-eedd-464e-a2a8-5fb45be639d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso_cl = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=\"auto\",\n",
    "    random_state=cfg.seed,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "iso_cl.fit(E_train)  \n",
    "\n",
    "raw = iso_cl.decision_function(E_gold)     # higher = more normal\n",
    "score_cl = -raw                            # higher = more anomalous\n",
    "\n",
    "gold_df = gold_df.copy()\n",
    "gold_df[\"score_cl\"] = score_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f20445-f307-4f61-9d95-ddb7a200b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_normalized_score(df: pd.DataFrame, col: str, out_col: str = \"score_norm_cl\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    s = df[col]\n",
    "    mask = s.notna()\n",
    "    if mask.sum() == 0:\n",
    "        df[out_col] = 0.0\n",
    "        return df\n",
    "    s_valid = s[mask]\n",
    "    s_min, s_max = float(s_valid.min()), float(s_valid.max())\n",
    "    if s_max == s_min:\n",
    "        df[out_col] = 0.5\n",
    "        return df\n",
    "    score = (s - s_min) / (s_max - s_min)\n",
    "    score[~mask] = 0.0\n",
    "    df[out_col] = score\n",
    "    return df\n",
    "\n",
    "gold_df = add_normalized_score(gold_df, col=\"score_cl\", out_col=\"score_norm_cl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff805238-d75b-4ed0-9e99-1de50513f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.loc[gold_df.index, \"score_norm_cl\"] = gold_df[\"score_norm_cl\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba543b4-2d09-4a02-98d5-5e5da73160af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df.loc[golden_df.index, \"score_norm\"] = golden_df[\"score_norm\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83aa95be-0b14-404c-9955-50c2c497346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"score_norm_merged\"] = (\n",
    "    cfg.score_merge_w * gold_df[\"score_norm_cl\"] +\n",
    "    (1.0 - cfg.score_merge_w) * gold_df[\"score_norm\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8051e889-3b69-4373-b067-d96f13afe8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERT_NODE_ID = \"ALERT_NODE\" \n",
    "\n",
    "def build_episode_graph_from_scores(df: pd.DataFrame, cfg: ContrastiveRunConfig) -> nx.DiGraph:\n",
    "    df = df.sort_values(\"timestamp\").copy()\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        nid = int(row[\"node_id\"])\n",
    "        G.add_node(\n",
    "            nid,\n",
    "            timestamp=row[\"timestamp\"],\n",
    "            stream=row[\"stream\"],\n",
    "            text=row[\"masked_message_cl\"],\n",
    "            actor_ip_anon=str(row.get(\"actor_ip_anon\", \"none\")),\n",
    "            host_anon=str(row.get(\"host_anon\", \"none\")),\n",
    "            score_cl=float(row.get(\"score_cl\", 0.0)),\n",
    "            score_norm=float(row.get(\"score_norm_merged\", 0.0))\n",
    "        )\n",
    "\n",
    "    def add_edges(key_col: str, max_gap: int, kind: str):\n",
    "        for key, group in df.groupby(key_col):\n",
    "            if str(key) in (\"none\", \"\", \"nan\", \"None\"):\n",
    "                continue\n",
    "            group = group.sort_values(\"timestamp\")\n",
    "            prev_nid, prev_ts = None, None\n",
    "            for _, r in group.iterrows():\n",
    "                nid = int(r[\"node_id\"])\n",
    "                ts = r[\"timestamp\"]\n",
    "                if prev_nid is not None:\n",
    "                    dt = (ts - prev_ts).total_seconds()\n",
    "                    if 0 <= dt <= max_gap:\n",
    "                        G.add_edge(prev_nid, nid, kind=kind, dt=float(dt))\n",
    "                prev_nid, prev_ts = nid, ts\n",
    "\n",
    "    if cfg.use_actor_edges:\n",
    "        add_edges(\"actor_ip_anon\", cfg.graph_max_gap_actor, \"actor_ip_temporal\")\n",
    "    if cfg.use_host_edges:\n",
    "        add_edges(\"host_anon\", cfg.graph_max_gap_host, \"host_temporal\")\n",
    "\n",
    "    return G\n",
    "\n",
    "G = build_episode_graph_from_scores(gold_df, cfg)\n",
    "print(\"Graph nodes/edges:\", G.number_of_nodes(), G.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e7f78b-83e1-4ee2-8593-8c7fd9d665d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_path = PROJECT_DIR / cfg.alert_csv_path\n",
    "alert_df = pd.read_csv(alert_path)\n",
    "alert_row = alert_df.iloc[0]\n",
    "\n",
    "alert_ts = pd.to_datetime(alert_row[\"@timestamp\"], utc=True)\n",
    "alert_src_ip = alert_row.get(\"source.ip\", None)\n",
    "alert_name   = str(alert_row.get(\"kibana.alert.rule.name\", \"\") or \"\")\n",
    "alert_reason = str(alert_row.get(\"kibana.alert.reason\", \"\") or \"\")\n",
    "alert_desc   = str(alert_row.get(\"description\", \"\") or \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99611281-96e7-4f0b-932a-fd7b5490b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "attached_log_ids = attach_alert_node(G, gold_df, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "save_json({\"attached_log_ids\": attached_log_ids}, RUN_DIR / \"alert_attach.json\")\n",
    "print(\"Attached node_ids:\", attached_log_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fce761-0611-4bd3-88be-963affdf80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert ALERT_NODE_ID in G.nodes\n",
    "\n",
    "succ = list(G.successors(ALERT_NODE_ID))\n",
    "print(\"Alert successors:\", succ[:10], \"count=\", len(succ))\n",
    "\n",
    "attached_nid = list(G.successors(ALERT_NODE_ID))[0]\n",
    "\n",
    "print(\"Outgoing from attached:\", G.out_degree(attached_nid))\n",
    "print(\"Incoming to attached:\", G.in_degree(attached_nid))\n",
    "\n",
    "anc = nx.ancestors(G, attached_nid) - {ALERT_NODE_ID}\n",
    "\n",
    "print(\"Ancestors of attached:\", len(anc))\n",
    "\n",
    "anc_df = gold_df.set_index(\"node_id\").loc[list(anc)]\n",
    "print(anc_df[\"stream\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b4e32b-8abc-4e9a-b76e-70ebd02327b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rca_walk_score(G: nx.DiGraph, cfg: ContrastiveRunConfig, alert_node_id: str = ALERT_NODE_ID):\n",
    "    alert_ts = G.nodes[alert_node_id][\"timestamp\"]\n",
    "\n",
    "    def get_score_norm(node_id):\n",
    "        if node_id == alert_node_id:\n",
    "            return 1.0\n",
    "        return float(G.nodes[node_id].get(\"score_norm\", 0.0) or 0.0)\n",
    "\n",
    "    def priority_key(score: float, dt: float):\n",
    "        adt = abs(dt)\n",
    "\n",
    "        if cfg.rca_priority_mode == \"baseline\":\n",
    "            return (-score, adt)\n",
    "\n",
    "        raise ValueError(f\"Unknown rca_priority_mode={cfg.rca_priority_mode}\")\n",
    "\n",
    "    selected = {alert_node_id}\n",
    "    visited  = {alert_node_id}\n",
    "    ranked   = [alert_node_id]\n",
    "\n",
    "    heap = []\n",
    "    for succ in G.successors(alert_node_id):\n",
    "        ts = G.nodes[succ][\"timestamp\"]\n",
    "        dt = (alert_ts - ts).total_seconds()\n",
    "\n",
    "        if dt > cfg.rca_max_back_seconds:\n",
    "            continue\n",
    "        if dt < -cfg.rca_forward_slack_seconds:\n",
    "            continue\n",
    "\n",
    "        score = get_score_norm(succ)\n",
    "        heapq.heappush(heap, (priority_key(score, dt), 1, succ))\n",
    "\n",
    "    while heap and len(selected) < cfg.rca_max_nodes:\n",
    "        (_, hops, node_id) = heapq.heappop(heap)\n",
    "\n",
    "        if node_id in visited:\n",
    "            continue\n",
    "        if cfg.rca_max_hops is not None and hops > cfg.rca_max_hops:\n",
    "            continue\n",
    "\n",
    "        visited.add(node_id)\n",
    "        selected.add(node_id)\n",
    "        ranked.append(node_id)\n",
    "\n",
    "        for pred in G.predecessors(node_id):\n",
    "            if pred in visited or pred == alert_node_id:\n",
    "                continue\n",
    "\n",
    "            ts = G.nodes[pred][\"timestamp\"]\n",
    "            dt = (alert_ts - ts).total_seconds()\n",
    "\n",
    "            if dt > cfg.rca_max_back_seconds:\n",
    "                continue\n",
    "            if dt < -cfg.rca_forward_slack_seconds:\n",
    "                continue\n",
    "\n",
    "            score = get_score_norm(pred)\n",
    "            heapq.heappush(heap, (priority_key(score, dt), hops + 1, pred))\n",
    "\n",
    "    subG = G.subgraph(selected).copy()\n",
    "    return subG, selected, ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771dc83e-8384-4acf-8578-818726b6b1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rca_walk_score_with_hops(G: nx.DiGraph, cfg: ContrastiveRunConfig, alert_node_id: str = \"ALERT_NODE\"):\n",
    "    alert_ts = G.nodes[alert_node_id][\"timestamp\"]\n",
    "\n",
    "    def get_score_norm(node_id):\n",
    "        if node_id == alert_node_id:\n",
    "            return 1.0\n",
    "        return float(G.nodes[node_id].get(\"score_norm\", 0.0) or 0.0)\n",
    "\n",
    "    def priority_key(score: float, dt: float):\n",
    "        adt = abs(dt)\n",
    "        if cfg.rca_priority_mode == \"baseline\":\n",
    "            return (-score, adt)\n",
    "        raise ValueError(f\"Unknown rca_priority_mode={cfg.rca_priority_mode}\")\n",
    "\n",
    "    selected = {alert_node_id}\n",
    "    visited  = {alert_node_id}\n",
    "    ranked   = [alert_node_id]\n",
    "\n",
    "    hop_of = {alert_node_id: 0}\n",
    "\n",
    "    heap = []\n",
    "    for succ in G.successors(alert_node_id):\n",
    "        ts = G.nodes[succ][\"timestamp\"]\n",
    "        dt = (alert_ts - ts).total_seconds()\n",
    "\n",
    "        if dt > cfg.rca_max_back_seconds:\n",
    "            continue\n",
    "        if dt < -cfg.rca_forward_slack_seconds:\n",
    "            continue\n",
    "\n",
    "        score = get_score_norm(succ)\n",
    "        heapq.heappush(heap, (priority_key(score, dt), 1, succ)) \n",
    "\n",
    "    while heap and len(selected) < cfg.rca_max_nodes:\n",
    "        (_, hops, node_id) = heapq.heappop(heap)\n",
    "\n",
    "        if node_id in visited:\n",
    "            continue\n",
    "        if cfg.rca_max_hops is not None and hops > cfg.rca_max_hops:\n",
    "            continue\n",
    "\n",
    "        visited.add(node_id)\n",
    "        selected.add(node_id)\n",
    "        ranked.append(node_id)\n",
    "        hop_of[node_id] = hops\n",
    "\n",
    "        for pred in G.predecessors(node_id):\n",
    "            if pred in visited or pred == alert_node_id:\n",
    "                continue\n",
    "\n",
    "            ts = G.nodes[pred][\"timestamp\"]\n",
    "            dt = (alert_ts - ts).total_seconds()\n",
    "\n",
    "            if dt > cfg.rca_max_back_seconds:\n",
    "                continue\n",
    "            if dt < -cfg.rca_forward_slack_seconds:\n",
    "                continue\n",
    "\n",
    "            score = get_score_norm(pred)\n",
    "            heapq.heappush(heap, (priority_key(score, dt), hops + 1, pred))\n",
    "\n",
    "    subG = G.subgraph(selected).copy()\n",
    "    return subG, selected, ranked, hop_of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9225ef16-db8e-4fcb-9d80-6ee1b12d09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_tmp = replace(cfg, rca_max_hops=None, rca_max_nodes=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc18dc-887f-4bc8-ace7-6925af48c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, selected, ranked, hop_of = rca_walk_score_with_hops(G, cfg_tmp)\n",
    "\n",
    "hop_s = pd.Series({k:v for k,v in hop_of.items() if k != ALERT_NODE_ID})\n",
    "\n",
    "print(\"Max hop (selected):\", int(hop_s.max()))\n",
    "print(hop_s.value_counts().sort_index().head(30)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ed7284-8d3c-4513-b1a5-8ed48d326dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_core_nodes = set(gold_df.loc[gold_df[\"gt_core\"], \"node_id\"].astype(int))\n",
    "gt_ext_nodes  = set(gold_df.loc[gold_df[\"gt_extended\"], \"node_id\"].astype(int))\n",
    "\n",
    "core_hops = [hop_of[n] for n in gt_core_nodes if n in hop_of]\n",
    "ext_hops  = [hop_of[n] for n in gt_ext_nodes  if n in hop_of]\n",
    "\n",
    "print(\"Core GT hops: max=\", max(core_hops), \"p90=\", pd.Series(core_hops).quantile(0.90))\n",
    "print(\"Ext  GT hops: max=\", max(ext_hops),  \"p90=\", pd.Series(ext_hops).quantile(0.90))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6694096-41af-4e4d-a0c0-fa4608a77e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "subG, selected_nodes, rca_ranked = rca_walk_score(G, cfg, alert_node_id=ALERT_NODE_ID)\n",
    "print(\"RCA subgraph:\")\n",
    "print(\"  Nodes:\", subG.number_of_nodes())\n",
    "print(\"  Edges:\", subG.number_of_edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a10849-71aa-432b-9036-c3e722cd0c35",
   "metadata": {},
   "source": [
    "## EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bec05fd-85da-4e71-92d3-d3935b81c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_by_id = gold_df.set_index(\"node_id\", drop=False)\n",
    "\n",
    "def _filter_ranked_nodes(rca_ranked: List[Any], alert_node_id: Any) -> List[int]:\n",
    "    return [int(n) for n in rca_ranked if n != alert_node_id and n != \"ALERT_NODE\"]\n",
    "\n",
    "def _ranked_to_items(ranked_nodes: List[int], df_by_id: pd.DataFrame, use_or_duplicates: bool, evidence_col: str=\"evidence_id\") -> List[str]:\n",
    "    if not use_or_duplicates:\n",
    "        return [str(n) for n in ranked_nodes]\n",
    "    ranked_items, seen = [], set()\n",
    "    for n in ranked_nodes:\n",
    "        eid = df_by_id.loc[n, evidence_col]\n",
    "        eid = \"missing_evidence\" if pd.isna(eid) else str(eid)\n",
    "        if eid not in seen:\n",
    "            seen.add(eid)\n",
    "            ranked_items.append(eid)\n",
    "    return ranked_items\n",
    "\n",
    "def _gt_items(df: pd.DataFrame, gt_col: str, use_or_duplicates: bool, evidence_col: str=\"evidence_id\") -> set:\n",
    "    gt_mask = df[gt_col].astype(bool)\n",
    "    if not use_or_duplicates:\n",
    "        return set(map(str, df.loc[gt_mask, \"node_id\"].tolist()))\n",
    "    return set(map(str, df.loc[gt_mask, evidence_col].dropna().tolist()))\n",
    "\n",
    "def _prf(returned_set: set, gt_set: set) -> Tuple[int, float, float, float]:\n",
    "    tp = len(returned_set & gt_set)\n",
    "    p = tp / max(1, len(returned_set))\n",
    "    r = tp / max(1, len(gt_set))\n",
    "    f1 = 0.0 if (p + r) == 0 else (2 * p * r / (p + r))\n",
    "    return tp, p, r, f1\n",
    "\n",
    "def _pr_at_k(ranked_items: List[str], gt_set: set, k: int) -> Tuple[int, float, float, int]:\n",
    "    effective_k = min(k, len(ranked_items))\n",
    "    topk = set(ranked_items[:effective_k])\n",
    "    tp = len(topk & gt_set)\n",
    "    p = tp / max(1, effective_k)\n",
    "    r = tp / max(1, len(gt_set))\n",
    "    return tp, p, r, effective_k\n",
    "\n",
    "def _hit_at_k(ranked_items: List[str], gt_set: set, k: int) -> Tuple[int, int]:\n",
    "    k_used = min(k, len(ranked_items))\n",
    "    if k_used == 0:\n",
    "        return 0, 0\n",
    "    topk = set(ranked_items[:k_used])\n",
    "    hit = 1 if len(topk & gt_set) > 0 else 0\n",
    "    return hit, k_used\n",
    "\n",
    "\n",
    "def evaluate_rca_episode(\n",
    "    df: pd.DataFrame,\n",
    "    df_by_id: pd.DataFrame,\n",
    "    rca_ranked: List[Any],\n",
    "    cfg: ContrastiveRunConfig,\n",
    "    use_or_duplicates: bool = True,\n",
    "    evidence_col: str = \"evidence_id\",\n",
    "    compute_hit_for: str = \"core\",  \n",
    ") -> Dict[str, Any]:\n",
    "\n",
    "    ranked_nodes = _filter_ranked_nodes(rca_ranked, ALERT_NODE_ID)\n",
    "    ranked_items = _ranked_to_items(ranked_nodes, df_by_id, use_or_duplicates, evidence_col=evidence_col)\n",
    "    returned_set = set(ranked_items)\n",
    "\n",
    "    gt_core = _gt_items(df, \"gt_core\", use_or_duplicates, evidence_col=evidence_col)\n",
    "    gt_ext  = _gt_items(df, \"gt_extended\", use_or_duplicates, evidence_col=evidence_col)\n",
    "\n",
    "    core_tp, core_p, core_r, core_f1 = _prf(returned_set, gt_core)\n",
    "    ext_tp,  ext_p,  ext_r,  ext_f1  = _prf(returned_set, gt_ext)\n",
    "\n",
    "    out = {\n",
    "        \"mode\": \"or_duplicates\" if use_or_duplicates else \"node_level\",\n",
    "        \"S_nodes\": len(ranked_nodes),\n",
    "        \"S_items\": len(returned_set),\n",
    "        \"returned_items_total_ranked\": len(ranked_items),\n",
    "        \"gt_core_size\": len(gt_core),\n",
    "        \"gt_ext_size\": len(gt_ext),\n",
    "        \"core_tp\": core_tp, \"core_precision\": core_p, \"core_recall\": core_r, \"core_f1\": core_f1,\n",
    "        \"ext_tp\":  ext_tp,  \"ext_precision\":  ext_p,  \"ext_recall\":  ext_r,  \"ext_f1\":  ext_f1,\n",
    "    }\n",
    "\n",
    "    for k in cfg.ks:\n",
    "        tp, p, r, k_used = _pr_at_k(ranked_items, gt_core, k)\n",
    "        out[f\"core_tp@{k}\"] = tp\n",
    "        out[f\"core_P@{k}\"] = p\n",
    "        out[f\"core_R@{k}\"] = r\n",
    "        out[f\"core_k_used@{k}\"] = k_used\n",
    "\n",
    "        tp, p, r, k_used = _pr_at_k(ranked_items, gt_ext, k)\n",
    "        out[f\"ext_tp@{k}\"] = tp\n",
    "        out[f\"ext_P@{k}\"] = p\n",
    "        out[f\"ext_R@{k}\"] = r\n",
    "        out[f\"ext_k_used@{k}\"] = k_used\n",
    "\n",
    "        if compute_hit_for in (\"core\"):\n",
    "            hit, k_used = _hit_at_k(ranked_items, gt_core, k)\n",
    "            out[f\"core_Hit@{k}\"] = hit\n",
    "            out[f\"core_hit_k_used@{k}\"] = k_used\n",
    "\n",
    "        if compute_hit_for in (\"ext\"):\n",
    "            hit, k_used = _hit_at_k(ranked_items, gt_ext, k)\n",
    "            out[f\"ext_Hit@{k}\"] = hit\n",
    "            out[f\"ext_hit_k_used@{k}\"] = k_used\n",
    "\n",
    "    return out\n",
    "\n",
    "metrics_or = evaluate_rca_episode(gold_df, golden_by_id, rca_ranked, cfg, use_or_duplicates=True, compute_hit_for=\"core\")\n",
    "metrics_node = evaluate_rca_episode(gold_df, golden_by_id, rca_ranked, cfg, use_or_duplicates=False, compute_hit_for=\"core\")\n",
    "\n",
    "save_json(metrics_or, RUN_DIR / \"metrics_or_duplicates.json\")\n",
    "save_json(metrics_node, RUN_DIR / \"metrics_node_level.json\")\n",
    "save_json({\"rca_ranked\": [str(x) for x in rca_ranked]}, RUN_DIR / \"rca_ranked.json\")\n",
    "\n",
    "metrics_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f4fb96-6f88-40fa-9845-1347d6d0b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_stream_breakdown(gold_df: pd.DataFrame, selected_node_ids: set, gt_col: str = \"gt_core\") -> pd.DataFrame:\n",
    "    df = gold_df.copy()\n",
    "    df[\"selected\"] = df[\"node_id\"].isin(selected_node_ids)\n",
    "    out = (\n",
    "        df.groupby(\"stream\")\n",
    "        .apply(lambda g: pd.Series({\n",
    "            \"tp\": int((g[gt_col].astype(bool) & g[\"selected\"]).sum()),\n",
    "            \"fp\": int((~g[gt_col].astype(bool) & g[\"selected\"]).sum()),\n",
    "            \"fn\": int((g[gt_col].astype(bool) & ~g[\"selected\"]).sum()),\n",
    "            \"selected\": int(g[\"selected\"].sum()),\n",
    "            \"gt_total\": int(g[gt_col].astype(bool).sum()),\n",
    "            \"total_logs\": len(g),\n",
    "        }))\n",
    "    )\n",
    "    out[\"precision\"] = out[\"tp\"] / (out[\"tp\"] + out[\"fp\"]).replace({0: pd.NA})\n",
    "    out[\"recall\"]    = out[\"tp\"] / (out[\"tp\"] + out[\"fn\"]).replace({0: pd.NA})\n",
    "    return out.fillna(0.0).sort_values([\"gt_total\", \"selected\"], ascending=False)\n",
    "\n",
    "def get_missed_and_extra_tables(gold_df: pd.DataFrame, selected_node_ids: set, gt_col: str=\"gt_core\", top_n: int=20):\n",
    "    df = gold_df.copy()\n",
    "    df[\"selected\"] = df[\"node_id\"].isin(selected_node_ids)\n",
    "    missed = df[(df[gt_col].astype(bool)) & (~df[\"selected\"])].copy()\n",
    "    extra  = df[(~df[gt_col].astype(bool)) & (df[\"selected\"])].copy()\n",
    "\n",
    "    cols = [\"timestamp\", \"stream\", \"masked_message_cl\", \"score_norm\", \"evidence_id\", \"node_id\"]\n",
    "    missed = missed.sort_values([\"score_norm\", \"timestamp\"], ascending=[False, True])[cols].head(top_n)\n",
    "    extra  = extra.sort_values([\"score_norm\", \"timestamp\"], ascending=[False, True])[cols].head(top_n)\n",
    "    return missed, extra\n",
    "\n",
    "selected_node_ids = set([int(n) for n in rca_ranked if n != ALERT_NODE_ID and n != \"ALERT_NODE\"])\n",
    "\n",
    "per_stream_core = per_stream_breakdown(gold_df, selected_node_ids, gt_col=\"gt_core\")\n",
    "per_stream_ext  = per_stream_breakdown(gold_df, selected_node_ids, gt_col=\"gt_extended\")\n",
    "\n",
    "missed_core, extra_non_core = get_missed_and_extra_tables(gold_df, selected_node_ids, gt_col=\"gt_core\", top_n=20)\n",
    "\n",
    "save_json(per_stream_core.to_dict(orient=\"index\"), RUN_DIR / \"per_stream_core.json\")\n",
    "save_json(per_stream_ext.to_dict(orient=\"index\"), RUN_DIR / \"per_stream_ext.json\")\n",
    "\n",
    "print(\"Saved diagnostics to:\", RUN_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6078dde9-9214-42cc-86cd-6713a5e9af3d",
   "metadata": {},
   "source": [
    "# EXPERIMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4b8ed8-ba75-43fd-bcdf-6e896876ffef",
   "metadata": {},
   "source": [
    "## EXPERIMENT A: Connectivity/Reachability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da290138-c19f-41b5-a180-e6b4fcf21f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reachable_by_walk(G: nx.DiGraph, cfg: ContrastiveRunConfig, alert_node_id: str = ALERT_NODE_ID) -> set[int]:\n",
    "    alert_ts = G.nodes[alert_node_id][\"timestamp\"]\n",
    "\n",
    "    def dt_ok(nid) -> bool:\n",
    "        ts = G.nodes[nid][\"timestamp\"]\n",
    "        dt = (alert_ts - ts).total_seconds()\n",
    "        if dt > cfg.rca_max_back_seconds:\n",
    "            return False\n",
    "        if dt < -cfg.rca_forward_slack_seconds:\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    visited = set([alert_node_id])\n",
    "    reachable = set()\n",
    "    q = deque()\n",
    "\n",
    "    for succ in G.successors(alert_node_id):\n",
    "        if succ == alert_node_id:\n",
    "            continue\n",
    "        if not dt_ok(succ):\n",
    "            continue\n",
    "        q.append((succ, 1))\n",
    "\n",
    "    while q:\n",
    "        nid, hops = q.popleft()\n",
    "\n",
    "        if nid in visited:\n",
    "            continue\n",
    "        if cfg.rca_max_hops is not None and hops > cfg.rca_max_hops:\n",
    "            continue\n",
    "\n",
    "        visited.add(nid)\n",
    "        if nid != alert_node_id:\n",
    "            reachable.add(int(nid))\n",
    "\n",
    "        for pred in G.predecessors(nid):\n",
    "            if pred in visited or pred == alert_node_id:\n",
    "                continue\n",
    "            if not dt_ok(pred):\n",
    "                continue\n",
    "            q.append((pred, hops + 1))\n",
    "\n",
    "    return reachable\n",
    "\n",
    "\n",
    "def _evidence_series_with_fallback(df: pd.DataFrame) -> pd.Series:\n",
    "    ev = df[\"evidence_id\"] if \"evidence_id\" in df.columns else pd.Series([pd.NA] * len(df), index=df.index)\n",
    "    ev = ev.where(ev.notna(), df[\"node_id\"].apply(lambda x: f\"node_{int(x)}\"))\n",
    "    ev = ev.astype(str)\n",
    "    ev = ev.replace({\"nan\": \"\", \"none\": \"\", \"None\": \"\"})\n",
    "    return ev\n",
    "\n",
    "\n",
    "def node_ids_to_evidence_set(gold_df: pd.DataFrame, node_ids: set[int]) -> set[str]:\n",
    "    sub = gold_df.loc[gold_df[\"node_id\"].isin(node_ids), [\"node_id\", \"evidence_id\"]].copy()\n",
    "    if sub.empty:\n",
    "        return set()\n",
    "    sub[\"evidence_id\"] = _evidence_series_with_fallback(sub)\n",
    "    ev_set = set(sub[\"evidence_id\"].tolist())\n",
    "    ev_set.discard(\"\")  \n",
    "    return ev_set\n",
    "\n",
    "\n",
    "def gt_evidence_set(gold_df: pd.DataFrame, gt_col: str) -> set[str]:\n",
    "    sub = gold_df.loc[gold_df[gt_col].astype(bool), [\"node_id\", \"evidence_id\"]].copy()\n",
    "    if sub.empty:\n",
    "        return set()\n",
    "    sub[\"evidence_id\"] = _evidence_series_with_fallback(sub)\n",
    "    ev_set = set(sub[\"evidence_id\"].tolist())\n",
    "    ev_set.discard(\"\")\n",
    "    return ev_set\n",
    "\n",
    "\n",
    "def reachable_recall_evidence(\n",
    "    gold_df: pd.DataFrame,\n",
    "    reachable_node_ids: set[int],\n",
    "    gt_col: str,\n",
    ") -> tuple[int, int, float]:\n",
    "    gt_ev = gt_evidence_set(gold_df, gt_col)\n",
    "    if len(gt_ev) == 0:\n",
    "        return 0, 0, 0.0\n",
    "    reachable_ev = node_ids_to_evidence_set(gold_df, reachable_node_ids)\n",
    "    hit = len(gt_ev & reachable_ev)\n",
    "    return hit, len(gt_ev), hit / len(gt_ev)\n",
    "\n",
    "def build_graph_and_attach(\n",
    "    gold_df: pd.DataFrame,\n",
    "    cfg_variant: ContrastiveRunConfig,\n",
    "    alert_ts: pd.Timestamp,\n",
    "    alert_src_ip,\n",
    "    alert_name: str,\n",
    "    alert_reason: str,\n",
    "    alert_desc: str,\n",
    ") -> nx.DiGraph:\n",
    "    Gv = build_episode_graph_from_scores(gold_df, cfg_variant)\n",
    "    attach_alert_node(Gv, gold_df, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "    return Gv\n",
    "\n",
    "def run_reachability_ablation_A_evidence(\n",
    "    gold_df: pd.DataFrame,\n",
    "    cfg_base: ContrastiveRunConfig,\n",
    "    alert_ts: pd.Timestamp,\n",
    "    alert_src_ip,\n",
    "    alert_name: str,\n",
    "    alert_reason: str,\n",
    "    alert_desc: str,\n",
    "    actor_gaps=(30, 60, 120, 300, 600), \n",
    "    host_gaps=(30, 60, 120, 300),        \n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    core_gt_ev = gt_evidence_set(gold_df, \"gt_core\")\n",
    "    ext_gt_ev  = gt_evidence_set(gold_df, \"gt_extended\")\n",
    "\n",
    "    edge_variants = [\n",
    "        (\"both_edges\", True, True,  cfg_base.graph_max_gap_actor, cfg_base.graph_max_gap_host),\n",
    "        (\"actor_only\", True, False, cfg_base.graph_max_gap_actor, cfg_base.graph_max_gap_host),\n",
    "        (\"host_only\",  False, True, cfg_base.graph_max_gap_actor, cfg_base.graph_max_gap_host),\n",
    "        (\"no_edges\",   False, False,cfg_base.graph_max_gap_actor, cfg_base.graph_max_gap_host),\n",
    "    ]\n",
    "\n",
    "    for name, ua, uh, a_gap, h_gap in edge_variants:\n",
    "        cfgv = replace(\n",
    "            cfg_base,\n",
    "            use_actor_edges=ua,\n",
    "            use_host_edges=uh,\n",
    "            graph_max_gap_actor=a_gap,\n",
    "            graph_max_gap_host=h_gap,\n",
    "        )\n",
    "\n",
    "        Gv = build_graph_and_attach(gold_df, cfgv, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "        reachable_nodes = compute_reachable_by_walk(Gv, cfgv, ALERT_NODE_ID)\n",
    "\n",
    "        reachable_ev = node_ids_to_evidence_set(gold_df, reachable_nodes)\n",
    "\n",
    "        core_hit, core_tot, rr_core = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_core\")\n",
    "        ext_hit,  ext_tot,  rr_ext  = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_extended\")\n",
    "\n",
    "        rows.append({\n",
    "            \"ablation\": \"edge_type\",\n",
    "            \"variant\": name,\n",
    "            \"use_actor_edges\": ua,\n",
    "            \"use_host_edges\": uh,\n",
    "            \"actor_gap_s\": int(a_gap),\n",
    "            \"host_gap_s\": int(h_gap),\n",
    "            \"nodes\": Gv.number_of_nodes(),\n",
    "            \"edges\": Gv.number_of_edges(),\n",
    "            \"reachable_nodes\": int(len(reachable_nodes)),\n",
    "            \"reachable_evidence\": int(len(reachable_ev)),\n",
    "            \"core_gt_evidence_total\": int(len(core_gt_ev)),\n",
    "            \"ext_gt_evidence_total\": int(len(ext_gt_ev)),\n",
    "            \"ReachRec_core_evidence\": rr_core,\n",
    "            \"core_reachable_evidence\": f\"{core_hit}/{core_tot}\",\n",
    "            \"ReachRec_ext_evidence\": rr_ext,\n",
    "            \"ext_reachable_evidence\": f\"{ext_hit}/{ext_tot}\",\n",
    "        })\n",
    "\n",
    "    for a_gap in actor_gaps:\n",
    "        cfgv = replace(\n",
    "            cfg_base,\n",
    "            use_actor_edges=True,\n",
    "            use_host_edges=True,\n",
    "            graph_max_gap_actor=int(a_gap),\n",
    "            graph_max_gap_host=int(cfg_base.graph_max_gap_host),\n",
    "        )\n",
    "\n",
    "        Gv = build_graph_and_attach(gold_df, cfgv, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "        reachable_nodes = compute_reachable_by_walk(Gv, cfgv, ALERT_NODE_ID)\n",
    "        reachable_ev = node_ids_to_evidence_set(gold_df, reachable_nodes)\n",
    "\n",
    "        core_hit, core_tot, rr_core = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_core\")\n",
    "        ext_hit,  ext_tot,  rr_ext  = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_extended\")\n",
    "\n",
    "        rows.append({\n",
    "            \"ablation\": \"actor_gap_sweep\",\n",
    "            \"variant\": f\"actor_gap={a_gap}\",\n",
    "            \"use_actor_edges\": True,\n",
    "            \"use_host_edges\": True,\n",
    "            \"actor_gap_s\": int(a_gap),\n",
    "            \"host_gap_s\": int(cfg_base.graph_max_gap_host),\n",
    "            \"nodes\": Gv.number_of_nodes(),\n",
    "            \"edges\": Gv.number_of_edges(),\n",
    "            \"reachable_nodes\": int(len(reachable_nodes)),\n",
    "            \"reachable_evidence\": int(len(reachable_ev)),\n",
    "            \"core_gt_evidence_total\": int(len(core_gt_ev)),\n",
    "            \"ext_gt_evidence_total\": int(len(ext_gt_ev)),\n",
    "            \"ReachRec_core_evidence\": rr_core,\n",
    "            \"core_reachable_evidence\": f\"{core_hit}/{core_tot}\",\n",
    "            \"ReachRec_ext_evidence\": rr_ext,\n",
    "            \"ext_reachable_evidence\": f\"{ext_hit}/{ext_tot}\",\n",
    "        })\n",
    "\n",
    "    for h_gap in host_gaps:\n",
    "        cfgv = replace(\n",
    "            cfg_base,\n",
    "            use_actor_edges=True,\n",
    "            use_host_edges=True,\n",
    "            graph_max_gap_actor=int(cfg_base.graph_max_gap_actor),\n",
    "            graph_max_gap_host=int(h_gap),\n",
    "        )\n",
    "\n",
    "        Gv = build_graph_and_attach(gold_df, cfgv, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "        reachable_nodes = compute_reachable_by_walk(Gv, cfgv, ALERT_NODE_ID)\n",
    "        reachable_ev = node_ids_to_evidence_set(gold_df, reachable_nodes)\n",
    "\n",
    "        core_hit, core_tot, rr_core = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_core\")\n",
    "        ext_hit,  ext_tot,  rr_ext  = reachable_recall_evidence(gold_df, reachable_nodes, \"gt_extended\")\n",
    "\n",
    "        rows.append({\n",
    "            \"ablation\": \"host_gap_sweep\",\n",
    "            \"variant\": f\"host_gap={h_gap}\",\n",
    "            \"use_actor_edges\": True,\n",
    "            \"use_host_edges\": True,\n",
    "            \"actor_gap_s\": int(cfg_base.graph_max_gap_actor),\n",
    "            \"host_gap_s\": int(h_gap),\n",
    "            \"nodes\": Gv.number_of_nodes(),\n",
    "            \"edges\": Gv.number_of_edges(),\n",
    "            \"reachable_nodes\": int(len(reachable_nodes)),\n",
    "            \"reachable_evidence\": int(len(reachable_ev)),\n",
    "            \"core_gt_evidence_total\": int(len(core_gt_ev)),\n",
    "            \"ext_gt_evidence_total\": int(len(ext_gt_ev)),\n",
    "            \"ReachRec_core_evidence\": rr_core,\n",
    "            \"core_reachable_evidence\": f\"{core_hit}/{core_tot}\",\n",
    "            \"ReachRec_ext_evidence\": rr_ext,\n",
    "            \"ext_reachable_evidence\": f\"{ext_hit}/{ext_tot}\",\n",
    "        })\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    out = out.sort_values(\n",
    "        [\"ablation\", \"ReachRec_core_evidence\", \"ReachRec_ext_evidence\", \"reachable_evidence\", \"edges\"],\n",
    "        ascending=[True, False, False, False, False],\n",
    "    )\n",
    "    return out\n",
    "\n",
    "ablation_A_evidence = run_reachability_ablation_A_evidence(\n",
    "    gold_df=gold_df,\n",
    "    cfg_base=cfg,\n",
    "    alert_ts=alert_ts,\n",
    "    alert_src_ip=alert_src_ip,\n",
    "    alert_name=alert_name,\n",
    "    alert_reason=alert_reason,\n",
    "    alert_desc=alert_desc,\n",
    ")\n",
    "\n",
    "display(ablation_A_evidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c47fc7-4a47-4133-9de5-a36debef6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_A_path = RUN_DIR / \"ablation_A_reachability.csv\"\n",
    "ablation_A_evidence.to_csv(ablation_A_path, index=False)\n",
    "print(\"Saved:\", ablation_A_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c6fc9a-43c8-4d48-a973-42b802425512",
   "metadata": {},
   "source": [
    "# EXPERIMENT B: Score Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f97a75e-4d6a-434d-9c32-fccac475a06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "results = []\n",
    "\n",
    "for w in weights:\n",
    "    gold_df[\"score_norm_merged\"] = w*gold_df[\"score_norm_cl\"] + (1-w)*gold_df[\"score_norm\"]\n",
    "    G = build_episode_graph_from_scores(gold_df, cfg) \n",
    "    attach_alert_node(G, gold_df, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "    subG, selected, rca_ranked = rca_walk_score(G, cfg)\n",
    "    metrics = evaluate_rca_episode(gold_df, gold_df.set_index(\"node_id\", drop=False), rca_ranked, cfg, use_or_duplicates=True)\n",
    "    metrics[\"w\"] = w\n",
    "    results.append(metrics)\n",
    "\n",
    "pd.DataFrame(results)[[\"w\",\"core_f1\",\"core_P@10\",\"core_R@10\",\"ext_f1\",\"ext_P@10\",\"ext_R@10\",\"ext_R@20\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b89fc-f5af-4b92-aad9-6245d720952a",
   "metadata": {},
   "source": [
    "# EXPERIMENT C: Walk budget + hop limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef765723-93fc-4c2e-a6ea-db0770d97f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_walk_eval(\n",
    "    G,\n",
    "    gold_df,\n",
    "    cfgv,\n",
    "    alert_node_id=ALERT_NODE_ID,\n",
    "    mode_name=\"contrastive\",\n",
    "    use_or_duplicates=True,\n",
    "    compute_hop_diagnostics=True,\n",
    "):\n",
    "    hop_of = None\n",
    "\n",
    "    if compute_hop_diagnostics and (\"rca_walk_score_with_hops\" in globals()):\n",
    "        subGv, selected_v, ranked_v, hop_of = rca_walk_score_with_hops(\n",
    "            G, cfgv, alert_node_id=alert_node_id\n",
    "        )\n",
    "    else:\n",
    "        subGv, selected_v, ranked_v = rca_walk_score(G, cfgv, alert_node_id=alert_node_id)\n",
    "\n",
    "    m = evaluate_rca_episode(\n",
    "        gold_df,\n",
    "        golden_by_id,\n",
    "        ranked_v,\n",
    "        cfgv,\n",
    "        use_or_duplicates=use_or_duplicates,\n",
    "    )\n",
    "\n",
    "    row = {\n",
    "        \"model\": mode_name,\n",
    "        \"rca_max_nodes\": int(cfgv.rca_max_nodes),\n",
    "        \"rca_max_hops\": (\"None\" if cfgv.rca_max_hops is None else int(cfgv.rca_max_hops)),\n",
    "        \"selected_nodes\": len([n for n in selected_v if n != alert_node_id]),\n",
    "    }\n",
    "    row.update(m)\n",
    "\n",
    "    # Hop diagnostics \n",
    "    if hop_of is not None:\n",
    "        sel_wo_alert = [n for n in selected_v if n != alert_node_id]\n",
    "        hops_vals = [hop_of.get(n) for n in sel_wo_alert if hop_of.get(n) is not None]\n",
    "        if len(hops_vals) > 0:\n",
    "            row[\"max_hop_selected\"] = int(np.max(hops_vals))\n",
    "            row[\"p90_hop_selected\"] = float(np.quantile(hops_vals, 0.90))\n",
    "            row[\"mean_hop_selected\"] = float(np.mean(hops_vals))\n",
    "        else:\n",
    "            row[\"max_hop_selected\"] = 0\n",
    "            row[\"p90_hop_selected\"] = 0.0\n",
    "            row[\"mean_hop_selected\"] = 0.0\n",
    "\n",
    "    return row\n",
    "\n",
    "\n",
    "def _sort_ablation(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    sort_cols = []\n",
    "    for c in [\"core_f1\", \"ext_f1\", \"core_P@10\", \"ext_P@10\"]:\n",
    "        if c in df.columns:\n",
    "            sort_cols.append(c)\n",
    "    for c in [\"S_items\", \"returned_items_total_ranked\", \"S_nodes\", \"selected_nodes\"]:\n",
    "        if c in df.columns:\n",
    "            sort_cols.append(c)\n",
    "\n",
    "    ascending = []\n",
    "    for c in sort_cols:\n",
    "        if c in (\"S_items\", \"returned_items_total_ranked\", \"S_nodes\", \"selected_nodes\"):\n",
    "            ascending.append(True)\n",
    "        else:\n",
    "            ascending.append(False)\n",
    "\n",
    "    if sort_cols:\n",
    "        return df.sort_values(sort_cols, ascending=ascending)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Hop sensitivity \n",
    "def run_hop_sensitivity(\n",
    "    G,\n",
    "    gold_df,\n",
    "    cfg_base,\n",
    "    alert_node_id=ALERT_NODE_ID,\n",
    "    fixed_max_nodes=5000,                    \n",
    "    max_hops_grid=(1, 2, 3, 5, 10, 25, 50, 100, 150, None),\n",
    "    mode_name=\"contrastive\",\n",
    "    compute_hop_diagnostics=True,\n",
    "):\n",
    "    rows = []\n",
    "    for max_hops in max_hops_grid:\n",
    "        cfgv = replace(\n",
    "            cfg_base,\n",
    "            rca_max_nodes=int(fixed_max_nodes),\n",
    "            rca_max_hops=max_hops,\n",
    "        )\n",
    "        row = _run_walk_eval(\n",
    "            G=G,\n",
    "            gold_df=gold_df,\n",
    "            cfgv=cfgv,\n",
    "            alert_node_id=alert_node_id,\n",
    "            mode_name=mode_name,\n",
    "            compute_hop_diagnostics=compute_hop_diagnostics,\n",
    "        )\n",
    "        row[\"ablation\"] = \"hop_sensitivity\"\n",
    "        rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "\n",
    "    def _hop_key(x):\n",
    "        return 10**9 if x == \"None\" else int(x)\n",
    "\n",
    "    if \"rca_max_hops\" in out.columns:\n",
    "        out = out.sort_values(\"rca_max_hops\", key=lambda s: s.map(_hop_key))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# Node budget sensitivity \n",
    "def run_budget_sensitivity(\n",
    "    G,\n",
    "    gold_df,\n",
    "    cfg_base,\n",
    "    alert_node_id=ALERT_NODE_ID,\n",
    "    max_nodes_grid=(25, 50, 100, 150, 200),\n",
    "    fixed_max_hops=None,                   \n",
    "    mode_name=\"contrastive\",\n",
    "    compute_hop_diagnostics=True,\n",
    "):\n",
    "    rows = []\n",
    "    for max_nodes in max_nodes_grid:\n",
    "        cfgv = replace(\n",
    "            cfg_base,\n",
    "            rca_max_nodes=int(max_nodes),\n",
    "            rca_max_hops=fixed_max_hops,\n",
    "        )\n",
    "        row = _run_walk_eval(\n",
    "            G=G,\n",
    "            gold_df=gold_df,\n",
    "            cfgv=cfgv,\n",
    "            alert_node_id=alert_node_id,\n",
    "            mode_name=mode_name,\n",
    "            compute_hop_diagnostics=compute_hop_diagnostics,\n",
    "        )\n",
    "        row[\"ablation\"] = \"budget_sensitivity\"\n",
    "        rows.append(row)\n",
    "\n",
    "    out = pd.DataFrame(rows)\n",
    "    if \"rca_max_nodes\" in out.columns:\n",
    "        out = out.sort_values(\"rca_max_nodes\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "hop_sens_contrastive = run_hop_sensitivity(\n",
    "    G=G,\n",
    "    gold_df=gold_df,\n",
    "    cfg_base=cfg,\n",
    "    alert_node_id=ALERT_NODE_ID,\n",
    "    fixed_max_nodes=5000,\n",
    "    max_hops_grid=(1, 2, 3, 5, 10, 25, 50, 100, 150, None),\n",
    "    mode_name=\"contrastive\",\n",
    "    compute_hop_diagnostics=True,\n",
    ")\n",
    "\n",
    "budget_sens_contrastive = run_budget_sensitivity(\n",
    "    G=G,\n",
    "    gold_df=gold_df,\n",
    "    cfg_base=cfg,\n",
    "    alert_node_id=ALERT_NODE_ID,\n",
    "    max_nodes_grid=(25, 50, 100, 150, 200),\n",
    "    fixed_max_hops=None,   \n",
    "    mode_name=\"contrastive\",\n",
    "    compute_hop_diagnostics=True,\n",
    ")\n",
    "\n",
    "display(_sort_ablation(hop_sens_contrastive))\n",
    "display(_sort_ablation(budget_sens_contrastive))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb052ab1-9cfe-42fe-91a3-6c4c768b705f",
   "metadata": {},
   "source": [
    "# EXPERIMENT D: SVD DIMENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1928bf4-92f9-4bc0-888a-67e4d74a6ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_contrastive_once(\n",
    "    svd_dim: int,\n",
    "    normalize_svd: bool = True,\n",
    "    normalize_proj: bool = True,\n",
    ") -> dict:\n",
    "\n",
    "    seed_everything(cfg.seed)\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=cfg.tfidf_max_features,\n",
    "        min_df=cfg.tfidf_min_df,\n",
    "        ngram_range=cfg.tfidf_ngram_range,\n",
    "    )\n",
    "    X_train_tfidf = tfidf.fit_transform(train_df[cfg.text_col])\n",
    "    X_gold_tfidf  = tfidf.transform(gold_df[cfg.text_col])\n",
    "\n",
    "    svd = TruncatedSVD(n_components=svd_dim, random_state=cfg.seed)\n",
    "    Z_train = svd.fit_transform(X_train_tfidf)\n",
    "    Z_gold  = svd.transform(X_gold_tfidf)\n",
    "\n",
    "    if normalize_svd:\n",
    "        norm = Normalizer(copy=False)\n",
    "        Z_train = norm.fit_transform(Z_train)\n",
    "        Z_gold  = norm.transform(Z_gold)\n",
    "\n",
    "    class PairDataset(Dataset):\n",
    "        def __init__(self, Z: np.ndarray, pairs: list[tuple[int,int]]):\n",
    "            self.Z = torch.tensor(Z, dtype=torch.float32)\n",
    "            self.pairs = pairs\n",
    "        def __len__(self): return len(self.pairs)\n",
    "        def __getitem__(self, idx):\n",
    "            i, j = self.pairs[idx]\n",
    "            return self.Z[i], self.Z[j]\n",
    "\n",
    "    ds_pairs = PairDataset(Z_train, pair_idx)\n",
    "    dl = DataLoader(ds_pairs, batch_size=cfg.batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    class Projector(nn.Module):\n",
    "        def __init__(self, in_dim: int, out_dim: int, normalize_out: bool = True):\n",
    "            super().__init__()\n",
    "            self.normalize_out = normalize_out\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, in_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(in_dim, out_dim),\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            z = self.net(x)\n",
    "            return F.normalize(z, dim=1) if self.normalize_out else z\n",
    "\n",
    "    def info_nce_loss(z1, z2, temperature: float):\n",
    "        B = z1.size(0)\n",
    "        labels = torch.arange(B, device=z1.device)\n",
    "        logits12 = (z1 @ z2.T) / temperature\n",
    "        logits21 = (z2 @ z1.T) / temperature\n",
    "        return 0.5 * (F.cross_entropy(logits12, labels) + F.cross_entropy(logits21, labels))\n",
    "\n",
    "    proj = Projector(svd_dim, cfg.proj_dim, normalize_out=normalize_proj).to(cfg.device)\n",
    "    opt = torch.optim.AdamW(proj.parameters(), lr=cfg.lr, weight_decay=1e-4)\n",
    "\n",
    "    proj.train()\n",
    "    for ep in range(cfg.epochs):\n",
    "        losses = []\n",
    "        for x1, x2 in dl:\n",
    "            x1, x2 = x1.to(cfg.device), x2.to(cfg.device)\n",
    "            z1, z2 = proj(x1), proj(x2)\n",
    "            loss = info_nce_loss(z1, z2, cfg.temperature)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(proj.parameters(), 1.0)\n",
    "            opt.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    proj.eval()\n",
    "    with torch.no_grad():\n",
    "        Zt = torch.tensor(Z_train, dtype=torch.float32, device=cfg.device)\n",
    "        Zg = torch.tensor(Z_gold, dtype=torch.float32, device=cfg.device)\n",
    "        E_train = proj(Zt).cpu().numpy()\n",
    "        E_gold  = proj(Zg).cpu().numpy()\n",
    "\n",
    "    iso_cl = IsolationForest(\n",
    "        n_estimators=100,\n",
    "        contamination=\"auto\",\n",
    "        random_state=cfg.seed,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    iso_cl.fit(E_train)\n",
    "    raw = iso_cl.decision_function(E_gold)   # higher = more normal\n",
    "    score_cl = -raw                          # higher = more anomalous\n",
    "\n",
    "    gold_local = gold_df.copy()\n",
    "    gold_local[\"score_cl\"] = score_cl\n",
    "\n",
    "    gold_local = add_normalized_score(gold_local, col=\"score_cl\", out_col=\"score_norm_cl\")\n",
    "\n",
    "    gold_local[\"score_norm_merged\"] = (\n",
    "        cfg.score_merge_w * gold_local[\"score_norm_cl\"] +\n",
    "        (1.0 - cfg.score_merge_w) * gold_local[\"score_norm\"]\n",
    "    )\n",
    "\n",
    "    G_local = build_episode_graph_from_scores(gold_local, cfg)\n",
    "\n",
    "    alert_path = PROJECT_DIR / cfg.alert_csv_path\n",
    "    alert_df_local = pd.read_csv(alert_path)\n",
    "    alert_row = alert_df_local.iloc[0]\n",
    "\n",
    "    alert_ts = pd.to_datetime(alert_row[\"@timestamp\"], utc=True)\n",
    "    alert_src_ip = alert_row.get(\"source.ip\", None)\n",
    "    alert_name   = str(alert_row.get(\"kibana.alert.rule.name\", \"\") or \"\")\n",
    "    alert_reason = str(alert_row.get(\"kibana.alert.reason\", \"\") or \"\")\n",
    "    alert_desc   = str(alert_row.get(\"description\", \"\") or \"\")\n",
    "\n",
    "    _ = attach_alert_node(G_local, gold_local, alert_ts, alert_src_ip, alert_name, alert_reason, alert_desc)\n",
    "\n",
    "    subG, selected_nodes, rca_ranked = rca_walk_score(G_local, cfg, alert_node_id=ALERT_NODE_ID)\n",
    "\n",
    "    df_by_id = gold_local.set_index(\"node_id\", drop=False)\n",
    "    metrics_or = evaluate_rca_episode(\n",
    "        gold_local, df_by_id, rca_ranked, cfg,\n",
    "        use_or_duplicates=True,\n",
    "        compute_hit_for=\"core\"  \n",
    "    )\n",
    "\n",
    "    metrics_or[\"svd_dim\"] = svd_dim\n",
    "    metrics_or[\"normalize_svd\"] = bool(normalize_svd)\n",
    "    metrics_or[\"normalize_proj\"] = bool(normalize_proj)\n",
    "\n",
    "    return metrics_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d929e7b-495c-4564-be6c-7fccc15743a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_dim_list = getattr(cfg, \"svd_dim_list\", [64, 128, 256, 512])\n",
    "\n",
    "rows = []\n",
    "t0 = time.time()\n",
    "\n",
    "for d in svd_dim_list:\n",
    "    print(f\"\\n=== Running svd_dim={d} (normalize_svd=True, normalize_proj={cfg.normalize_proj}) ===\")\n",
    "    m = run_contrastive_once(\n",
    "        svd_dim=d,\n",
    "        normalize_svd=True,\n",
    "        normalize_proj=cfg.normalize_proj,\n",
    "    )\n",
    "    rows.append({\n",
    "        \"svd_dim\": d,\n",
    "        \"normalize_svd\": m[\"normalize_svd\"],\n",
    "        \"normalize_proj\": m[\"normalize_proj\"],\n",
    "        \"S_items\": m[\"S_items\"],\n",
    "        \"core_f1\": m[\"core_f1\"],\n",
    "        \"ext_f1\": m[\"ext_f1\"],\n",
    "        \"core_P@10\": m.get(\"core_P@10\", None),\n",
    "        \"ext_P@10\": m.get(\"ext_P@10\", None),\n",
    "        \"core_Hit@10\": m.get(\"core_Hit@10\", None),\n",
    "    })\n",
    "\n",
    "df_dim = pd.DataFrame(rows).sort_values(\"svd_dim\").reset_index(drop=True)\n",
    "print(f\"\\nDone. Total time: {(time.time()-t0)/60:.1f} min\")\n",
    "df_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee52e2d-c153-4830-97bd-918f79b55bb4",
   "metadata": {},
   "source": [
    "# EXPERIMENT E: Post-SVD Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6748e61f-e20b-4244-83ba-b0d6ae5d618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "t0 = time.time()\n",
    "\n",
    "for norm_flag in [True, False]:\n",
    "    print(f\"\\n=== Running normalize_svd={norm_flag} (svd_dim=256, normalize_proj={cfg.normalize_proj}) ===\")\n",
    "    m = run_contrastive_once(\n",
    "        svd_dim=256,\n",
    "        normalize_svd=norm_flag,\n",
    "        normalize_proj=cfg.normalize_proj,  \n",
    "    )\n",
    "    rows.append({\n",
    "        \"svd_dim\": m[\"svd_dim\"],\n",
    "        \"normalize_svd\": m[\"normalize_svd\"],\n",
    "        \"normalize_proj\": m[\"normalize_proj\"],\n",
    "        \"S_items\": m[\"S_items\"],\n",
    "        \"core_f1\": m[\"core_f1\"],\n",
    "        \"ext_f1\": m[\"ext_f1\"],\n",
    "        \"core_P@10\": m.get(\"core_P@10\", None),\n",
    "        \"ext_P@10\": m.get(\"ext_P@10\", None),\n",
    "        \"core_Hit@10\": m.get(\"core_Hit@10\", None),\n",
    "    })\n",
    "\n",
    "df_norm = pd.DataFrame(rows).sort_values([\"svd_dim\", \"normalize_svd\"], ascending=[True, False]).reset_index(drop=True)\n",
    "print(f\"\\nDone. Total time: {(time.time()-t0)/60:.1f} min\")\n",
    "df_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea144b9-c46d-454d-8a49-45e8e0515a42",
   "metadata": {},
   "source": [
    "# EXPERIMENT F: Post-Projection Head Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc786e-b683-46cb-84de-72388cac63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for norm_svd in [True, False]:\n",
    "    for norm_proj in [True, False]:\n",
    "        print(f\"\\n=== svd_norm={norm_svd}, proj_norm={norm_proj} (svd_dim=256) ===\")\n",
    "        m = run_contrastive_once(\n",
    "            svd_dim=256,\n",
    "            normalize_svd=norm_svd,\n",
    "            normalize_proj=norm_proj,\n",
    "        )\n",
    "        rows.append({\n",
    "            \"normalize_svd\": m[\"normalize_svd\"],\n",
    "            \"normalize_proj\": m[\"normalize_proj\"],\n",
    "            \"S_items\": m[\"S_items\"],\n",
    "            \"core_f1\": m[\"core_f1\"],\n",
    "            \"ext_f1\": m[\"ext_f1\"],\n",
    "            \"core_P@10\": m.get(\"core_P@10\", None),\n",
    "            \"ext_P@10\": m.get(\"ext_P@10\", None),\n",
    "            \"core_Hit@10\": m.get(\"core_Hit@10\", None),\n",
    "        })\n",
    "\n",
    "df_norm_2x2 = pd.DataFrame(rows).sort_values([\"normalize_svd\", \"normalize_proj\"], ascending=[False, False]).reset_index(drop=True)\n",
    "df_norm_2x2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6244d5-bc00-4dfc-a47c-3b0edd8ee9d9",
   "metadata": {},
   "source": [
    "# EXTRA METRIC: AUPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4665436-7c9f-403e-a248-b178255bfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def auprc_over_reachable_evidence(df, reachable_nodes, score_col, gt_col):\n",
    "    df = gold_df.copy()\n",
    "    df[\"evidence_id\"] = _evidence_series_with_fallback(df)\n",
    "\n",
    "    reachable_ev = node_ids_to_evidence_set(df, reachable_nodes)\n",
    "\n",
    "    # restrict to reachable evidence items\n",
    "    df = df[df[\"evidence_id\"].isin(reachable_ev)][[\"evidence_id\", score_col, gt_col]].copy()\n",
    "\n",
    "    # collapse duplicates to evidence-level:\n",
    "    ev = df.groupby(\"evidence_id\").agg(\n",
    "        y_score=(score_col, \"max\"),\n",
    "        y_true=(gt_col, \"max\"),\n",
    "    )\n",
    "\n",
    "    y_true = ev[\"y_true\"].astype(int).to_numpy()\n",
    "    y_score = ev[\"y_score\"].astype(float).to_numpy()\n",
    "\n",
    "    if y_true.sum() == 0 or y_true.sum() == len(y_true):\n",
    "        return None, len(ev), int(y_true.sum())\n",
    "\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    return ap, len(ev), int(y_true.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3163f5-4bdb-4d27-8105-08dbacf9eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "ap_b_core, pool, pos = auprc_over_reachable_evidence(\n",
    "    df=golden_df,\n",
    "    reachable_nodes=reachable_nodes,\n",
    "    score_col=\"score_norm\",\n",
    "    gt_col=\"gt_core\",\n",
    ")\n",
    "ap_b_ext, _, _ = auprc_over_reachable_evidence(\n",
    "    df=golden_df,\n",
    "    reachable_nodes=reachable_nodes,\n",
    "    score_col=\"score_norm\",\n",
    "    gt_col=\"gt_extended\",\n",
    ")\n",
    "\n",
    "# Contrastive\n",
    "ap_c_core, _, _ = auprc_over_reachable_evidence(\n",
    "    df=gold_df,\n",
    "    reachable_nodes=reachable_nodes,\n",
    "    score_col=\"score_norm_cl\",   # <- change to your real column\n",
    "    gt_col=\"gt_core\",\n",
    ")\n",
    "ap_c_ext, _, _ = auprc_over_reachable_evidence(\n",
    "    df=gold_df,\n",
    "    reachable_nodes=reachable_nodes,\n",
    "    score_col=\"score_norm_cl\",\n",
    "    gt_col=\"gt_extended\",\n",
    ")\n",
    "\n",
    "print(f\"Baseline AUPRC core/ext:     {ap_b_core:.4f} / {ap_b_ext:.4f}   pool={pool} pos={pos}\")\n",
    "print(f\"Contrastive AUPRC core/ext:  {ap_c_core:.4f} / {ap_c_ext:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c8736-3690-4dc6-afb6-3b1acf0d0e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def pr_curve_over_reachable_evidence(\n",
    "    golden_df,\n",
    "    reachable_node_ids,\n",
    "    score_col: str,\n",
    "    gt_col: str,\n",
    "):\n",
    "    df = gold_df.copy()\n",
    "    df[\"evidence_key\"] = _evidence_series_with_fallback(df)\n",
    "\n",
    "    df = df[df[\"node_id\"].isin(reachable_node_ids)].copy()\n",
    "    grp = df.groupby(\"evidence_key\", sort=False)\n",
    "    y_true = grp[gt_col].max().astype(int).to_numpy()\n",
    "    y_score = grp[score_col].max().to_numpy()\n",
    "\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    ap = average_precision_score(y_true, y_score)\n",
    "    prevalence = float(y_true.mean())\n",
    "    return precision, recall, ap, prevalence, len(y_true), int(y_true.sum())\n",
    "\n",
    "def plot_pr(ax, title, pr_b, pr_c):\n",
    "    (p_b, r_b, ap_b, prev_b, pool_b, pos_b) = pr_b\n",
    "    (p_c, r_c, ap_c, prev_c, pool_c, pos_c) = pr_c\n",
    "\n",
    "    ax.plot(r_b, p_b, label=f\"Baseline (AP={ap_b:.4f})\")\n",
    "    ax.plot(r_c, p_c, label=f\"Contrastive (AP={ap_c:.4f})\")\n",
    "    ax.hlines(prev_b, 0, 1, linestyles=\"--\", label=f\"Random (p={prev_b:.4f})\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Recall\")\n",
    "    ax.set_ylabel(\"Precision\")\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend()\n",
    "    axes[0].set_ylim(0, 0.10)\n",
    "    axes[1].set_ylim(0, 0.10)\n",
    "\n",
    "pr_b_core = pr_curve_over_reachable_evidence(golden_df, reachable_nodes, \"score_norm\",    \"gt_core\")\n",
    "pr_c_core = pr_curve_over_reachable_evidence(gold_df, reachable_nodes, \"score_norm_cl\", \"gt_core\")\n",
    "\n",
    "pr_b_ext  = pr_curve_over_reachable_evidence(golden_df, reachable_nodes, \"score_norm\",    \"gt_extended\")\n",
    "pr_c_ext  = pr_curve_over_reachable_evidence(gold_df, reachable_nodes, \"score_norm_cl\", \"gt_extended\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_pr(axes[0], \"PR curve over reachable evidence (core GT)\", pr_b_core, pr_c_core)\n",
    "plot_pr(axes[1], \"PR curve over reachable evidence (extended GT)\", pr_b_ext, pr_c_ext)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (master_thesis_env)",
   "language": "python",
   "name": "master_thesis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
